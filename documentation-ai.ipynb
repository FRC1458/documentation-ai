{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain gpt4all bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "from gpt4all import GPT4All, Embed4All\n",
    "\n",
    "import pickle\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subpages(base_url):\n",
    "    # Initialize an empty list to store the subpage URLs\n",
    "    subpages = []\n",
    "\n",
    "    # Send a request to the base_url to get the HTML content\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags (<a>) in the HTML\n",
    "    anchor_tags = soup.find_all('a')\n",
    "\n",
    "    # Extract the href attribute from each anchor tag and form the absolute URLs\n",
    "    for tag in anchor_tags:\n",
    "        href = tag.get('href')\n",
    "        if href:\n",
    "            subpage_url = urljoin(base_url, href)\n",
    "            subpages.append(subpage_url)\n",
    "\n",
    "    # Return the list of subpage URLs\n",
    "    return subpages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = 'https://github.wpilib.org/allwpilib/docs/release/java/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpages_list = get_subpages(urls)\n",
    "s_list = []\n",
    "for page in subpages_list:\n",
    "    if page.startswith(\"https://github.wpilib.org/allwpilib/docs/release/java/edu/wpi/first\"):\n",
    "        s_list.append(page)\n",
    "s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_page = ['https://github.wpilib.org/allwpilib/docs/release/java/edu/wpi/first/apriltag/AprilTagDetection.html']\n",
    "loaders = UnstructuredURLLoader(urls=example_page)\n",
    "data = loaders.load()\n",
    "text_splitter = CharacterTextSplitter(separator='\\n',\n",
    "                                      chunk_size=1000,\n",
    "                                      chunk_overlap=200)\n",
    "\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain\\embeddings\\llamacpp.py:85\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[0;32m     87\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llama \u001b[39m=\u001b[39m LlamaCppEmbeddings(model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./ggml-model-q4_0.bin\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m VectorStore_OpenAI \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(docs, embedder)\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfaiss_store_openai.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain\\embeddings\\llamacpp.py:89\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     87\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n\u001b[0;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     90\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load Llama model from path: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "llama = LlamaCppEmbeddings(model_path='./ggml-model-q4_0.bin')\n",
    "VectorStore_OpenAI = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "with open(\"faiss_store_openai.pkl\", \"wb\") as f:\n",
    "  pickle.dump(VectorStore_OpenAI, f)\n",
    "with open(\"faiss_store_openai.pkl\", \"rb\") as f:\n",
    "  VectorStore = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
